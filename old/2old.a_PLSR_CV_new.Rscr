#!/opt/R-3.2.3/bin//Rscript

# ================================================================================ 
# Step 2a: Process all HRUs for each variable (cross-validated); Used to determine 
#          which variable is most important for the predicted month and hru
#
#  NEW: testing different PLSR process (post Balaji meeting 7/20)
#
# Created by S. Baker, June 2018
# ================================================================================ 
rm(list=ls())

# ## === Input Data
source("/home/sabaker/s2s/analysis/scripts/cfsv2_analysis/post_process/plsr/0_control_PLSRmdl.R")
beg.time         = Sys.time()
preds            = 1:length(var_ls)        # preds to include in prediction
Nparallel       = 10

print('starting...')
print(beg.time)

## === libraries
suppressMessages(library(lubridate))
suppressMessages(library(ncdf4))
suppressMessages(library(abind))
suppressMessages(library(OceanView))
suppressMessages(library(pls))
suppressMessages(library(dplyr))
suppressMessages(library(plyr))
library(parallel)

##### ===== Load CFSv2 Data 
setwd(out_model_dir)
load(file = file_data)
nms = names(data_out)

## extract objects
pred_cfs = data_out[[nms[1]]]
grid = data_out[[nms[2]]]
var_nms = c(data_out[[nms[3]]], 'hru_cfs')
date_cfs = data_out[[nms[4]]]
rm(data_out)

## get lengths of vars 
Ntime           = length(pred_cfs[1,1,1,])
Nlat            = length(pred_cfs[1,1,,1])
Nlon            = length(pred_cfs[1,,1,1])
Nvar            = length(pred_cfs[,1,1,1])

## get overlapping indexes for lagged analysis
t_ind <- NULL
for (itime in 1:Ntime) {
  t_i = data.frame(matrix(which(as.Date(date_cfs) >= (as.Date(date_cfs[itime]) - l) & as.Date(date_cfs) <= (as.Date(date_cfs[itime]))),
                          nrow = 1))
  colnames(t_i) <- paste0('X', seq(1:length(t_i)))
  t_ind = rbind.fill(t_ind, t_i)
}

### === average lagged variables loop through time
# takes a long time
# check if analysis is lagged and lagged data is saved yet
if (l > 0) {
  if (!file.exists(file_lag_in)){
    pred_avg = array(NA,dim=c(Nvar,Nlon,Nlat,Ntime))
    for (itime in 1:Ntime) {
      pred_avg[,,,itime]  = apply(pred_cfs[,,, na.omit(as.numeric(t_ind[itime,])) ], c(1,2,3),mean)
    }
    saveRDS(pred_avg, file = file_lag_in )
    
    pred_cfs = pred_avg
    rm(pred_avg)
  } else {
    pred_cfs = readRDS(file_lag_in)
  }
}

##### ===== Load NLDAS Data (obs)
setwd(TP_data)
file_nld   = nc_open(paste0('nldas.1_2.', pred_var, '.nc')) 
hru_v_nld   = ncvar_get(file_nld, "hru")  # hru

## read time and find overlapping index
time = ncvar_get(file_nld, "time")
date_nld = as.POSIXct((time - 7) * 86400, 
                      origin = '1980-01-01', tz = 'utc') #timeseries of day 1 of obs
ind_overlap = inner_join(data.frame(date = date_cfs), 
                         data.frame(date = date_nld, ind = 1:length(date_nld)), by = 'date')

## read variable and convert units
nld_all   = ncvar_get(file_nld, pred_var)[, unique(ind_overlap$ind)]
if (pred_var == 'prate') { nld_all = nld_all * 24 } ## convert mm/hr to mm/d
nc_close(file_nld)


### === Load CFSv2 T & P prediction on HRU
file_pred   = nc_open(paste0('cfsv2.', wk, '.', pred_var, '.nc')) 
time_pred   = ncvar_get(file_pred, "time")
hru_v_cfs   = ncvar_get(file_pred, "hru")  # hru

## get date and mearge
date_pred = as.POSIXct(as.Date(as.POSIXct(time_pred - 6*86400, 
                                          origin = '1970-01-01', tz = 'utc'))) #timeseries of day 1 in cst
df_date        = data.frame(date = date_pred, ind = 1:length(date_pred))
ind_overlap    = left_join(ind_overlap, df_date, by = 'date')
cfs_pred_all   = ncvar_get(file_pred, pred_var)[ , ind_overlap$ind.y]
if ( pred_var == 'prate' ) { cfs_pred_all = cfs_pred_all * 86400 } ## convert kg/m^2/s to mm/d
cfs_pred_all   = data.frame(date = ind_overlap$date, var = as.data.frame(t(cfs_pred_all)))
cfs_pred_all   = cfs_pred_all %>% group_by(date) %>% dplyr::summarise_all(funs(mean)) # daily average
cfs_date       = cfs_pred_all$date
cfs_pred_all$date <- NULL

nc_close(file_pred)

### ===  get groups for validation
date_v          = unique(ind_overlap$date)
date_v          = data.frame(date = date_v, mon = month(date_v), ind = 1:length(date_v))
date_val        = filter(date_v, mon == pred_mon)
Nval            = nrow(date_val)
Ngroups         = ceiling(Nval/Ntime_sub)
cutgroup        = sapply(1:Ngroups, function(j) c(rep(j,Ntime_sub)))[1:Nval] #not random
xvalindex       = split(date_val$ind, cutgroup) # validate on these indexes
yvalindex       = split(1:Nval, cutgroup) 
Nsegments       = length(xvalindex)-1


##### ===== Arrange data for PLSR ===== ##### 
Alldata <- ind_pred <- NULL
for(ipred in preds){ # Start loop over selected predictors

  Ny = length(na.omit(unique(grid[ipred, 1,])))
  Nx = length(na.omit(unique(grid[ipred, 2,])))
  
  pred_i = array(0,dim=c(Ntime,Nx*Ny))
  
  for(ilon in 1:Nx){
    for(ilat in 1:Ny){
      pred_i[,(Ny * (ilon - 1) + ilat)]  = pred_cfs[ipred,ilon,ilat,]
    }
  }
  
  # remove NAs (from SST or any other vars)
  t = which(!is.na(pred_i[1,]), TRUE)
  pred_i = pred_i[,t]
  
  # get beginning of data
  ind_pred = c(ind_pred, ncol(Alldata)) #last ind of var
  
  # For SST - get predictor in correct format
  if (var_nms[ipred] == 'sst') {
    df = na.omit(data.frame(y= grid[ipred ,1, ],  x = grid[ipred ,2,]))
    ## sst has NA's that need to be removed
    grid_sst = df[t,] #save grid
  }
  
  # bind data
  pred_i = scale(pred_i) # do for each var seperately
  Alldata   <- cbind(Alldata, pred_i) 
} 

## replace sst grid
if (any(var_nms[preds] == 'sst')) {
  grid_n = array(NA, dim = c(2, length(grid[1,1,])))
  grid_n[1, ] = c(grid_sst[,1], rep(NA, (length(grid[1,1,]) - nrow(grid_sst))))
  grid_n[2, ] = c(grid_sst[,2], rep(NA, (length(grid[1,1,]) - nrow(grid_sst))))
  grid[which(var_nms == 'sst'),,] = grid_n
}

grid_ls = list(grid = grid, ind = ind_pred, var_nms = var_nms)

# setwd(out_model_dir)
# saveRDS(grid_ls, file = 'grid_data.rds')
input_data = list(nld_all = nld_all, cfs_pred_all = cfs_pred_all, ind_pred = ind_pred,
                  hru_v_nld = hru_v_nld, var_nms = var_nms, Nval = Nval, Nvar = Nvar,
                  Ngroups = Ngroups, xvalindex = xvalindex, Alldata = Alldata,
                  yvalindex = yvalindex, date_val = date_val, cfs_date = cfs_date)
setwd(allVar_model_dir)
saveRDS(input_data, paste0('plsr_input_allVarsHRUs_',pred_var, '_wk', wk, '_mon', pred_mon,'.rds'))
rm(list=setdiff(ls(), 'input_data'))

## read data back in

#setwd()
source("0_control_PLSRmdl.R")
input_data = readRDS("plsr_input_allVarsHRUs_tmp2m_wk3_4_mon1.rds")

nld_all = input_data$nld_all
cfs_pred_all = input_data$cfs_pred_all
ind_pred = input_data$ind_pred
hru_v_nld <- hru_v_cfs <- input_data$hru_v_nld
var_nms = input_data$var_nms
Nval = input_data$Nval
Nvar = input_data$Nvar
Ngroups = input_data$Ngroups
xvalindex = input_data$xvalindex
yvalindex = input_data$yvalindex
Alldata = input_data$Alldata
date_val = input_data$date_val
cfs_date = input_data$cfs_date


####  =====  INPUT
# HRU to Analyze
hru_i = hru_v_nld[1]
# predictors to analyze
ipred = c(5, 8)

# extract hru coordinates for NLDAS and CFSv2
hruInd_nld = which(hru_v_nld == hru_i)
hruInd_cfs = which(hru_v_cfs == hru_i)

nld_df = nld_all[hruInd_nld,]
cfs_pred_df = as.vector(cfs_pred_all[,hruInd_cfs])

## Scale NLDAS variable
nldasVar      = scale(nld_df)
Ymean         = mean(nld_df)     # Mean nldasVar during training period
Ysd           = sd(nld_df)       # Standard deviation of nldasVar during training period

## get index of predictors in Alldata matrix
ind = NULL
for (i in ipred) {
  ind = c(ind, (ind_pred[i] + 1):(ind_pred[(i +1)]))
}

##### ===== Predicat - loop over each potential predictor ===== ##### 
print(paste(pred_var, '- PLSR in CV - Forecasting', wk, 'wk -', month.abb[pred_mon]))
print(paste('HRU:', hru_nm, '=', hru_i, '; number', hruInd_nld))
print(paste('Lagged =', l, '- hru ', hru_i))
print(paste('Evaluating predictor', var_nms[ipred]))

df_out <- matrix(NA, ncol = 8, nrow = Nval*Nvar)
load   <- array(NA, dim = c(1, Ngroups ,length(ind), Ncomp))
df_cv  <- matrix(NA, ncol = 2, nrow = Nval) #NULL

## Cross-Validation - loop through segments
igroup = 1
  # for(igroup in 1:Ngroups){ # Start CV loop over groups
    
    ## Define predictand
    Ypredictand   = nldasVar[-xvalindex[[igroup]]]        # Observed nldasVar for the training period
    Yverif        = nldasVar[xvalindex[[igroup]]] #  [date_val$ind]  
    
    ## Extract predictors (separate training from verification dataset) & only ipred predictors
    Xtrain      = Alldata[, ind][-xvalindex[[igroup]],]
    Xverif      = Alldata[, ind][ xvalindex[[igroup]],]
    
    ## Fit PLSR model and predict! - from 'PredictPLSRCrossVal.R' function
    mydf        = data.frame(Ypredictand)
    mydf$X      = Xtrain
    
    # library(plsRglm)
    # test = plsR(Ypredictand ~ X, data = mydf,  nt = Ncomp, typeVC = 'none') # no cross validation here
    #             
    # 
    
    # slower than using plsr - supposed to be similar results, but isnt 
    plsrmod     = pls::pcr(Ypredictand ~ X, ncomp = Ncomp, data = mydf,
                            model = TRUE, x = TRUE, y = TRUE, #validation = 'CV',
                            segments=Nsegments) # takes a long time and worse results
    
    ## perform PLSR
    plsrmod     = pls::plsr(Ypredictand ~ X, ncomp = Ncomp, data = mydf, 
                            model = TRUE, x = TRUE, y = TRUE, #validation = 'CV',
                            segments=Nsegments, segment.type='consecutive', method='simpls')
    
    scores  = plsrmod$scores         # x score (x components?)
    loads   = plsrmod$loadings       # x loadings (x gridded value importance)
    coefs   = plsrmod$coefficients   # regression coefficients
    proj    = plsrmod$projection
    
    ## predict Y and save
    Yhat_ver2    = predict(plsrmod, ncomp = Ncomp, Xverif, se.fit=T)
    df_cv[yvalindex[[igroup]],] = cbind(Yverif, Yhat_ver)
    
    
    Xverif %*% B[-1,,i] + rep(B[1,,i], each = nobs)
    
    ## === prediction function
    newdata = Xverif
    object = plsrmod
    ncomp = Ncomp
    newX = newdata
    B <- coef(object, ncomp = ncomp, intercept = TRUE)
    dPred <- dim(B)
    dPred[1] <- dim(newX)[1]
    dnPred <- dimnames(B)
    dnPred[1] <-
      if(is.null(dimnames(newX))) list(NULL) else dimnames(newX)[1]
    pred <- array(dim = dPred, dimnames = dnPred)
    for (i in seq(along = ncomp))
      pred[,,i] <- newX %*% B[-1,,i] + rep(B[1,,i], each = nobs)
    return(pred) # same as predict function output
    
    
    
    
    
    ###########
    ## - Take the leading PCs and perform CCA
    ###  Fit a regression for each canonical variate

    # PCA of Xs --- takes soooooo long!! crashed computer
    zsvd=svd(var(Xtrain)) # Eigen decomposition
    x_eof = zsvd$u
    x_pcs_full = Xtrain %*% zsvd$u # Principal Components
    # Eigen Values - fraction variance 
    lambdas=(zsvd$d/sum(zsvd$d))
    x_PC = x_pcs_full[,1:Ncomp]
    #sstpcfull = pcs
    
    ### PCA on Y
    rainMean = apply(Ypredictand, 2, mean)
    rainSd = apply(Ypredictand, 2, sd)
    y_1=scale(Ypredictand)
    zs=var(Ypredictand)
    
    ### Eigen decomposition
    zsvd=svd(var(Ypredictand))
    y_eof = zsvd$u
    
    ### Principal Components
    y_pcs_full = y_1 %*% zsvd$u
    y_PC = y_pcs_full[,1:Ncomp]
    
    ### Perform CCA
    M=dim(x_PC)[2]
    J=dim(y_PC)[2]
    J=min(M,J)
    N = length(y_PC[,1])
    Qx1 = qr.Q(qr(x_PC))
    Qy1 = qr.Q(qr(y_PC))
    T11 = qr.R(qr(x_PC))
    T22 = qr.R(qr(y_PC))
    VV=t(Qx1) %*% Qy1
    BB = svd(VV)$v
    AA = svd(VV)$u
    BB = solve(T22) %*% svd(VV)$v * sqrt(N-1)
    wm1 = y_PC %*% BB
    AA = solve(T11) %*% svd(VV)$u * sqrt(N-1)
    vm1 = x_PC %*% AA
    cancorln = svd(VV)$d[1:J] #canonical correlation
    Fyy = var(y_PC) %*% BB
    
    ### Predict the Rain PCS
    betahat = solve(t(AA) %*% t(x_PC)%*% x_PC %*% AA) %*% t(AA) %*% t(x_PC) %*% y_PC
    ypred=x_PC %*% AA %*% betahat
    
    ### first npc PCs from the PC forecast above and the remaining PCs are set to
    ## their means - i.e., 0
    
    N1 = dim(y_1)[2]-(npc)
    rainPCpred = cbind(ypred,matrix(rep(0,N),ncol=N1,nrow=N))
    
    ## back transform to get the rainfall field
    #rainpred = rainPCpred %*% raineof
    #rainPCpred = cbind(ypred,rainpcfull[,npc1:dim(y_1)[2]])
    
    ##  Keep only the first npc Eigen Vectors and set rest to zero
    E = matrix(0,nrow=dim(y_1)[2],ncol=dim(y_1)[2])
    E[,1:npc]=raineof[,1:npc]
    
    ### first npc PCs from the PC forecast above and the remaining PCs are set to
    ## their means - i.e., 0
    
    N1 = dim(y_1)[2]-(npc)
    rainPCpred = cbind(ypred,matrix(rep(0,N),ncol=N1,nrow=N))
    
    ## back transform to get the rainfall field anomalies
    rainpred =  rainPCpred %*% t(E)
    
    ### rescale the rainfall
    rainpred=t(t(rainpred)*rainSd + rainMean)
    
    ### correlate the forecasted PCs with the historical PCs
    pccor = diag(cor(ypred,rainPC[,1:npc]))
    
    ## correlate the forecasted rainfall with the historical rainfall
    raincor = diag(cor(rainpred,raindata))
    
    # collect loadings for each training period
    load[1, igroup,,] = matrix(loadings(plsrmod)[,1:Ncomp], ncol = Ncomp)
    
    
    
    
    
    
  # } # group CV loop

  # collect data from predictor
  df = data.frame(pred   = rep(ipred, Nval),
                  date   = date_val$date,
                  ind    = 1:Nval,
                  Y_plsr = df_cv[,2] * Ysd + Ymean, 
                  Y_cfs  = dplyr::left_join(date_val, cbind.data.frame(date = cfs_date, var = cfs_pred_df), by = 'date')$var,
                  Y_nld  = df_cv[,1] * Ysd + Ymean)
  df$Y_plsr   = ifelse(df$Y_plsr <0, 0, df$Y_plsr) # remove negatives
  df$res_plsr = df$Y_plsr - df$Y_nld
  df$res_cfs  = df$Y_cfs - df$Y_nld

  
  # print cor
  print(paste('Cor predictor =', round(cor(df$Y_plsr, df$Y_nld), digits = 4), 
              '; Raw Cor =', round(cor(df$Y_cfs, df$Y_nld), digits = 4)))
  
# }

## save results
list(hru = hru_i, df = df_out, load = load)


## create list to save
#data = list(output = data_hrus, vars_nms = var_nms[preds], ind_pred = ind_pred, grid = grid)

## save results
setwd(allHRU_model_dir)
# saveRDS(data_hrus, file = file_all)
print(paste('saved...', file_all))

print(paste('Simulation Time:', end.time))
